{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBK3gVakZD2m"
      },
      "source": [
        "# Clustering \n",
        "\n",
        "*Based on materials by Brandon Rose, Jacob Eisenstein, and Eun Seo Jo et al.*\n",
        "\n",
        "Last class, we learned about classification, which assumes we have a test set of data that we have already categorized: in our case, we categorized by gender. When we run models with labeled data, we call it _supervised learning_.\n",
        "\n",
        "But what happens when we don't have labeled data? Is it possible to learn anything? \n",
        "\n",
        "This scenario is known as *unsupervised learning*, and we will see that it is very possible to learn about the underlying structure of unlabeled observations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoKzGw-eZD2n"
      },
      "source": [
        "## An example\n",
        "\n",
        "Here’s a scenario: \n",
        "\n",
        "Suppose you scrape thousands of news articles. And suppose you want to organize, or cluster, those documents by their prevalence of language about hurricanes vs elections. You could choose a group of words relate to hurricanes and make the x-axis the frequency with which those words appear in the documents. You could do the same for words related to elections on the y-axis. We might imagine the articles clustering into four groups: one for documents that are largely about a hurricane, another for documents largely about a election, a third for documents about neither, and a fourth for documents, however unlikely, about both.\n",
        "\n",
        "These clusters represent an underlying structure of the data. But it's still supervised, because we've labeled words as belonging to categories and used them to organize documents.\n",
        "\n",
        "Unsupervised learning applies the same basic idea, but in a high-dimensional space with one dimension for every word. As such, the space can’t be visualized. But the goal is the same as in two-dimensions: identify an underlying structure of the observed data, such that documents cluster, and each cluster is internally coherent. \n",
        "\n",
        "**Clustering algorithms** are capable of finding such structure automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOmgbcbMZD2o"
      },
      "source": [
        "## Enter k-means clustering \n",
        "\n",
        "Clustering algorithms assign each data point—for us, each document—to a discrete cluster. One of the best known clustering algorithms is k-means, an iterative algorithm that maintains a cluster assignment for each instance, and a central (\"mean\") location for each cluster. K-means iterates between updates to the assignments and the centers:\n",
        "\n",
        "1. each instance is placed in the cluster with the closest center;\n",
        "\n",
        "2. each center is recomputed as the average over points in the cluster.\n",
        "\n",
        "Let's watch [the start of a little video](https://www.youtube.com/watch?v=4b5d3muPQmA) to see how it works. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLuoMCd7ZD2o"
      },
      "source": [
        "**NOTE:** An important property of k-means is that the converged solution depends on the initialization, and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDFm08SgZD2o"
      },
      "source": [
        "## Let's get to it! ##\n",
        "\n",
        "In this example, we're going to use k-means clustering to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list), a corpus created by Brandon Rose. See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjguVD3qZD2o"
      },
      "source": [
        "Let's start with the imports. They include old friends: re, BeautifulSoup, nltk, pandas, sklearn, a veritable journey through our semester:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB4EF4n5ZD2o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup \n",
        "import re \n",
        "from sklearn import feature_extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus\n",
        "\n",
        "Now let's load in our corpus. Here, we're going to import four files that each map to each other: film titles, links to the films on imdb, wikipedia synopses of the films, and the associated genres for each.\n"
      ],
      "metadata": {
        "id": "eDahqp2hgcJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "import gdown\n",
        "\n",
        "# Download the title list file - a list of film titles, in order of top-ness\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=15FqA2-VKSoGdT3DT2IBiSCeIROQup2LH', quiet=False) \n",
        "\n",
        "# Download the url list - a list of URLS to each film on IMDB, in the same order as the first\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1LMbHmHlAZXWJN0KAIxbvfy5zB_6sAwio', quiet=False) \n",
        "\n",
        "# Download the synopses file - a list of synopses of each film, in the same order as both files above, with \"BREAKS HERE\" separating each entry\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1BDRU4wcG6Qd1zaQwyfQouufw2hmAEOKr', quiet=False) \n",
        "\n",
        "# Download the genres file - a list of associated genres, one film per line, in the same order as above\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1Phf-rcmQXneEiUra5z_l5Eky7x6FDITk', quiet=False) \n"
      ],
      "metadata": {
        "id": "xA7gNLycgbcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt4v3pOuZD2p"
      },
      "source": [
        "## Corpus Pre-Proccesing ##\n",
        "\n",
        "Now let's pre-process our corpus for clustering. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTZnrh8iZD2p"
      },
      "outputs": [],
      "source": [
        "# import four files that each map to each other: titles, links, wikipedia synopses, and associated dgenres\n",
        "\n",
        "# first, the list of film titles, in order of top-ness\n",
        "titles = open('title_list.txt').read().split('\\n')\n",
        "\n",
        "# ensures that only the top 100 titles are read in\n",
        "titles = titles[:100]\n",
        "\n",
        "# a list of URLS to each film on IMDB, in the same order as the first\n",
        "links = open('link_list_imdb.txt').read().split('\\n')\n",
        "links = links[:100]\n",
        "\n",
        "# a list of synopses of each film, in the same order as both files above, with \"BREAKS HERE\" separating each entry\n",
        "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_wiki = synopses_wiki[:100]\n",
        "\n",
        "synopses_clean_wiki = []\n",
        "\n",
        "for text in synopses_wiki:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    # strips html formatting and converts to unicode\n",
        "    synopses_clean_wiki.append(text)\n",
        "\n",
        "synopses_wiki = synopses_clean_wiki\n",
        "\n",
        "# now the list of associated genres, one film per line, in the same order as above\n",
        "genres = open('genres_list.txt').read().split('\\n')\n",
        "genres = genres[:100]\n",
        "\n",
        "print(str(len(titles)) + ' titles')\n",
        "print(str(len(links)) + ' links')\n",
        "print(str(len(synopses_wiki)) + ' synopses')\n",
        "print(str(len(genres)) + ' genres')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-UOVHlZD2p"
      },
      "source": [
        "Let's see what movies we have and their genres:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3ewuE2fZD2p"
      },
      "outputs": [],
      "source": [
        "for x in range(99):\n",
        "    print(titles[x], genres[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC-EMcX3ZD2p"
      },
      "source": [
        "Let's check out the first synopsis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RekvTdMPZD2p"
      },
      "outputs": [],
      "source": [
        "print(synopses_wiki[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUU5x-yHZD2p"
      },
      "outputs": [],
      "source": [
        "# now let's get the imdb synopses as well\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1NofscgV8eWPhgM4QxSAN3cyIhwENqIk4', quiet=False) \n",
        "\n",
        "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_imdb = synopses_imdb[:100]\n",
        "\n",
        "synopses_clean_imdb = []\n",
        "\n",
        "for text in synopses_imdb:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_imdb.append(text)\n",
        "\n",
        "synopses_imdb = synopses_clean_imdb\n",
        "\n",
        "# and check out the first\n",
        "synopses_imdb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS_d3i4YZD2p"
      },
      "outputs": [],
      "source": [
        "# make a list with the two sets of synopses—from imdb and wikipedia\n",
        "\n",
        "synopses = []\n",
        "\n",
        "for i in range(len(synopses_wiki)):\n",
        "    item = synopses_wiki[i] + synopses_imdb[i]\n",
        "    synopses.append(item)\n",
        "\n",
        "# see what one looks like\n",
        "print(synopses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxUMyAVyZD2q"
      },
      "source": [
        "### For document clustering, some people like to stem first ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAIA3zKtZD2q"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install textblob # an alternative to spaCy\n",
        "from textblob import TextBlob\n",
        "\n",
        "def textblob_tokenizer(str_input):\n",
        "    blob = TextBlob(str_input.lower())\n",
        "    tokens = blob.words\n",
        "    words = [token.stem() for token in tokens]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybYxXxDJZD2q"
      },
      "source": [
        "## Now tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM8VA2YEZD2q"
      },
      "source": [
        "Before we can cluster our documents—the movie synopses—we need to prepare them as vectors. We're going to prepare them as *tf-idf* vectors because they provide more effective clusters than other ways of counting words, for example raw word counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS4DsG-wZD2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, \n",
        "                                    ngram_range=(1,3), min_df=.2, max_df=0.8) #note new params\n",
        "\n",
        "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(synopses)\n",
        "\n",
        "print(tfidf_vectorizer_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbR-r2kDZD2q"
      },
      "source": [
        "We have 100 vectors, one for each synopsis, and each vector has values for 496 features. What are those features? Let's find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umqg8Wv1ZD2q"
      },
      "outputs": [],
      "source": [
        "# get our feature names for future reference \n",
        "# terms = tfidf_vectorizer.get_feature_names()\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZSLxT7yZD2q"
      },
      "outputs": [],
      "source": [
        "# get the first vector out (for the first synopsis) just to see what it looks like \n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
        "\n",
        "# place tf-idf values in a pandas data frame\n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
        "\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W-LABZDZD2q"
      },
      "source": [
        "This is the vector for **The Godfather**. It scores high on 'don', 'family', and 'son'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx4oln-gZD2q"
      },
      "source": [
        "## And on to the k-means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73pJ0tfZD2q"
      },
      "source": [
        "Using our tf-idf vectors, we can now run the k-means clustering algorithm. Remember that k-means initializes with a pre-determined number of clusters. Let's choose 5. \n",
        "\n",
        "**NOTE:** One of the metrics commonly used to compare results across different values of k is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing k will always decrease this metric, to the extreme of reaching zero when k is the same as the number of data points. Thus, this metric cannot be used as the sole target. \n",
        "\n",
        "Instead, mean distance to the centroid as a function of k is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine the ideal value for k.\n",
        "\n",
        "A number of other techniques exist for validating k, but that's the most common one. \n",
        "\n",
        "In any case, back to our clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77j52_aYZD2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 5\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know \n",
        "\n",
        "km.fit(tfidf_vectorizer_vectors)\n",
        "\n",
        "# km.labels_ gives you the cluster assignments\n",
        "clusters = km.labels_.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHQoPX50ZD2q"
      },
      "outputs": [],
      "source": [
        "# dump our clusters into a dataframe\n",
        "films = { 'title': titles, 'idx': list(range(100)), 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
        "\n",
        "film_df = pd.DataFrame(films, columns = ['title', 'idx', 'cluster', 'genre'])\n",
        "\n",
        "film_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-4oHUx5ZD2q"
      },
      "outputs": [],
      "source": [
        "# find out how many films are in each cluster\n",
        "film_df['cluster'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AwP2si9ZD2q"
      },
      "outputs": [],
      "source": [
        "# find the top terms per cluster\n",
        "\n",
        "# this orders by the distance of each term from the center\n",
        "# (cluster_centers_ returns an array of [n_clusters, n_features] )\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster \" + str(i) + \" top words: \")\n",
        "    top_terms = \"\"\n",
        "   \n",
        "    for ind in order_centroids[i, :10]:\n",
        "        top_terms += terms[ind] + \", \"\n",
        "  \n",
        "    print(top_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY2a95xYZD2r"
      },
      "outputs": [],
      "source": [
        "# find all films in each cluster    \n",
        "for i in range(num_clusters):  \n",
        "    print(\"Titles in cluster \" + str(i) + \": \")\n",
        "    cluster_titles = \"\"\n",
        "\n",
        "    # create new df of only the specific cluster\n",
        "    # remember boolean selection! \n",
        "    cluster_df = film_df[ film_df[\"cluster\"] == i ]\n",
        " \n",
        "    # create series of titles assoc w/ that cluster \n",
        "    for title in cluster_df['title']: \n",
        "        cluster_titles += title + \", \"\n",
        "\n",
        "    print(cluster_titles + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnF_o5elZD2r"
      },
      "outputs": [],
      "source": [
        "# Find the top 10 films in each cluster    \n",
        "for i in range(num_clusters):  \n",
        "    \n",
        "    # returns array w/ distances to the centroid i \n",
        "    dist = km.transform(tfidf_vectorizer_vectors)[:, i] \n",
        "    \n",
        "    # return indices for top 10 for that cluster \n",
        "    idxs = np.argsort(dist)[::][:10] \n",
        "    \n",
        "    print(\"Top 10 films in cluster \" + str(i) + \": \")\n",
        "    cluster_top_films = \"\"\n",
        "\n",
        "    # look up film title by index\n",
        "    for idx in idxs: \n",
        "        cluster_top_films += film_df.loc[idx,'title'] + \", \"\n",
        "        \n",
        "    print(cluster_top_films + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OElgtonZD2r"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMg-kyNcZD2r"
      },
      "source": [
        "How well do you think the clustering worked? Why?\n",
        "\n",
        "**Hint: Use the labeled genres as validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THCDqAXdZD2r"
      },
      "source": [
        "ANSWER HERE\n",
        "* *\n",
        "* *\n",
        "* *\n",
        "* *\n",
        "* *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZHWe772ZD2r"
      },
      "source": [
        "## Your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOuyp6H5ZD2r"
      },
      "source": [
        "Now it's your turn! See if you can replicate the pipeline above but with our corpus of NYT obituaries. I've started you out by reading in the files, as we did last class, but I've left most of the remaining work for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg1OFLjCZD2r"
      },
      "source": [
        "### 1. Read in the files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First step, read in the files\n",
        "\n",
        "# import libraries for extracting filenames from filepaths\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Download the zip file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1G0Aeg8dzZGPOCNFZ77U-s9CfEnR8efbB', quiet=False) "
      ],
      "metadata": {
        "id": "85im95bvj_S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip it \n",
        "!unzip NYT-Obituaries.zip"
      ],
      "metadata": {
        "id": "TXnZJAXTkCOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxg0VDz4ZD2r"
      },
      "outputs": [],
      "source": [
        "# Now, collect filepaths as files\n",
        "directory = \"NYT-Obituaries/\" # your path will be different!\n",
        "files = glob.glob(f\"{directory}/*.txt\")\n",
        "\n",
        "# and collect obit titles, which are also the final section of the filepaths\n",
        "obit_titles = [Path(file).stem for file in files]\n",
        "\n",
        "# print out the number of files to make sure it worked\n",
        "print(\"Number of files: \" + str(len(files)))\n",
        "\n",
        "# print out the title of the first obit to make sure that part worked\n",
        "print(\"Title of first file: \" + obit_titles[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBLo79weZD2r"
      },
      "source": [
        "### 2. Pre-process the files into a single list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1UvFiJ1ZD2r"
      },
      "outputs": [],
      "source": [
        "# As we know, the sk-learn tf-idf vectorizer creates its vectors from a list,\n",
        "# with each item in the list containing the text of a single document\n",
        "\n",
        "# Here, we need to get from our list of filenames -- called files -- created in the\n",
        "# list above, to another list, which we can call obits, which contains the text of \n",
        "# each file\n",
        "\n",
        "obits = []\n",
        "\n",
        "for file in files:\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        obits.append(text)\n",
        "\n",
        "# see what one looks like\n",
        "print(obits[0])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6j-qSJGZD2r"
      },
      "source": [
        "### 3. Create the tf-idf vectors and the list of features\n",
        "\n",
        "Note that you can modify this code from the tf-idf cell up above, but remember that we're using a new list call \"obits\" rather than the \"synopses\" one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpYn3NvCZD2r"
      },
      "outputs": [],
      "source": [
        "# First, create the tf-idf vectors. Look up above for code that instantiates sk-learn's \n",
        "# tf-idf vectorizer and then creates the vectors \n",
        "\n",
        "obit_tfidf_vectorizer = # complete\n",
        "\n",
        "obit_tfidf_vectorizer_vectors = # complete\n",
        "\n",
        "# check the shape of the vectors\n",
        "print(obit_tfidf_vectorizer_vectors.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Vqb9DFZD2r"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFU0oDSPZD2r"
      },
      "outputs": [],
      "source": [
        "# Second, create the list of features for future use\n",
        "# HINT: Use the .get_feature_names_out() method to query the vectorizer\n",
        "\n",
        "obit_terms = # complete \n",
        "\n",
        "obit_terms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGIf3_uvZD2r"
      },
      "source": [
        "**Data check**. Use the code below to print out the terms associated with the first document, the obituary of Hitler. Note that you may need to modify the variable names if you used different ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hfV-eYsMZD2r"
      },
      "outputs": [],
      "source": [
        "# get the first vector out (for the first synopsis) just to see what it looks like \n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
        "\n",
        "# place tf-idf values in a pandas data frame\n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
        "\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9EjQdJ8ZD2r"
      },
      "source": [
        "### 4. Do the clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdQOraegZD2r"
      },
      "outputs": [],
      "source": [
        "# First, do the clustering. Look up above for code that instantiates sk-learn's k-means \n",
        "# clustering model and then fits it to the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv_6A3U2ZD2s"
      },
      "outputs": [],
      "source": [
        "## Then, create a list of the cluster assignments \n",
        "### HINT: Use the labels_ attribte to get the cluster assignments from the model, and convert it to a list\n",
        "\n",
        "obit_clusters = # complete\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-N3c4gUZD2s"
      },
      "source": [
        "**Data check:** Use the code below to dump the cluster assignments into a dataframe for future use. Note that you may need to modify the variable names if you used different ones.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4af_UJvQZD2s"
      },
      "outputs": [],
      "source": [
        "# dump our clusters into a dataframe\n",
        "nyt_obits = { 'title': obit_titles, 'idx': range(obit_tfidf_vectorizer_vectors.shape[0]), 'cluster': obit_clusters }\n",
        "\n",
        "obit_clusters_df = pd.DataFrame(nyt_obits, columns = ['title', 'idx', 'cluster' ])\n",
        "\n",
        "obit_clusters_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66s15dEBZD2s"
      },
      "source": [
        "### 5. Examine the results\n",
        "\n",
        "First, explore the top terms per cluster. Look up above for code that you can modify for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x-AGy8UZD2s"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFJlX8TqZD2s"
      },
      "source": [
        "Then, see which obituaries were associated with each cluster. Look up above for code you can modify for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1W6FISXZD2s"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKZfMQ59ZD2s"
      },
      "source": [
        "Last, look at the top 10 obits in each cluster. Look above for code you can modify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fo_mNJKZD2s"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7qj-UFrZD2s"
      },
      "source": [
        "## BONUS: Dimensionality Reduction with T-SNE\n",
        "\n",
        "You have maybe heard of **t-sne** (is it TEA SNEA? or TAE SNAE..).  This is newer dimension reduction method that emphasizes visual convenience. The con of t-sne is that it is not as easily interpretable as k-means. It's also non-deterministic -- you'll get different but similar results everytime. But I thought you should play with it since it is widely used in machine learning today.\n",
        "\n",
        "Example projects: [hip hop](https://pudding.cool/2017/09/hip-hop-words/) and [wikipedia political ideologies](https://genekogan.com/works/wiki-tSNE/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJSLmtJIZD2s"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "\n",
        "embed = tsne.fit_transform(tfidf_vectorizer_vectors.toarray()) # visualizing the films \n",
        "# for the obits, it would be `obit_tfidf_vectorizer_vectors` instead\n",
        "\n",
        "xs, ys = zip(*embed) # we'll use these coords below, after we set up our plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev5HNw0ZZD2s"
      },
      "source": [
        "## Visualizing document clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWk9y77OZD2s"
      },
      "outputs": [],
      "source": [
        "#set up colors per clusters using a dict\n",
        "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
        "\n",
        "#set up cluster names using a dict\n",
        "cluster_names = {0: 'Cluster 0', \n",
        "                 1: 'Cluster 1', \n",
        "                 2: 'Cluster 2', \n",
        "                 3: 'Cluster 3', \n",
        "                 4: 'Cluster 4'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjtB6Y7FZD2s"
      },
      "outputs": [],
      "source": [
        "#create data frame that has the result of the t-sne plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) # here's where the coords come in \n",
        "# again for obits, we'd change out the label var to \"obit_clusters\" and the title var to \"obit_titles\"\n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRCZvsEBZD2s"
      },
      "outputs": [],
      "source": [
        "# set up plot\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
        "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "# iterate through groups to layer the plot\n",
        "# note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return \n",
        "# the appropriate color/label\n",
        "for name, group in groups:\n",
        "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
        "    ax.set_aspect('auto')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom=False,      # ticks along the bottom edge are off\n",
        "        top=False,         # ticks along the top edge are off\n",
        "        labelbottom=False)\n",
        "    ax.tick_params(\\\n",
        "        axis= 'y',         # changes apply to the y-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        left=False,      # ticks along the bottom edge are off\n",
        "        top=False,         # ticks along the top edge are off\n",
        "        labelleft=False)\n",
        "    \n",
        "ax.legend(numpoints=1)  #show legend with only 1 point\n",
        "\n",
        "#add label in x,y position with the label as the film title\n",
        "for i in range(len(df)):\n",
        "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
        "    \n",
        "plt.show() #show the plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SShBn76bZD2s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}