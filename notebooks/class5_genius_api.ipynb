{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWHdhEEY4uxC"
      },
      "source": [
        "## Fun with the Genius API\n",
        "\n",
        "As we have already begun to discuss, many web sites and organizations offer web APIs, which can be a rich source for textual data. We're going to go over how one real-world API works, the [Genius API](https://docs.genius.com/). By introducing you to this API, you'll learn the tools necessary to sign up for, query, and interpret APIs from other providers (as you will be asked to do in your first quiz for this course).\n",
        "\n",
        "### Signing up for an API Key (aka Client Access Token)\n",
        "\n",
        "Before you can use the Genius API, you need to sign up for a \"client access token,\" which is another name for an API key, as was discussed in the homework. Do so by filling out the [New API Client form](https://genius.com/api-clients/new). If you don't yet have an account on Genius.com, you'll be prompted to register first. \n",
        "\n",
        "The next questions don't really apply to our use in class, but they're required to get your token. You'll be prompted to fill out a short form about the \"App\" that you need the Genius API for. You only need to fill out \"App Name\" and \"App Website URL.\" You can enter any words you want in \"App Name.\" Similarly, you can enter any URL in the \"App Website URL,\" like so:\n",
        "\n",
        "<img src=\"http://lklein.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-15-at-11.21.25-AM.png\" style=\"width:400px\">\n",
        "\n",
        "When you click \"Save,\" you'll be given a series of API keys: a \"Client ID\" and a \"Client Secret.\" To generate your \"Client Access Token,\" which is the API key that we'll be using in this notebook, you need to click \"Generate Access Token\".\n",
        "\n",
        "The token is just a string of letters and numbers. It'll look something like this:\n",
        "\n",
        "    6617c28c371f0a138f7912a35365564afe538605\n",
        "    \n",
        "That's your \"key\" for that API. Whenever you make a request to that API, you'll need to include your key in the request. The exact method for including the key will be explained below. (Note: the key above is just something I made up; it's not a valid key; don't try using it in actual requests.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C8XYC184uxE"
      },
      "outputs": [],
      "source": [
        "# sign up for a client access token from Genius"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMRsyB3w4uxF"
      },
      "source": [
        "copy and paste your \"Client Access Token\" into the quotation marks below, and run the cell to save your variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdNjYh9G4uxF"
      },
      "outputs": [],
      "source": [
        "client_access_token = \" YOURS HERE \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kwhyI7F4uxG"
      },
      "source": [
        "### Making an API Request\n",
        "\n",
        "Remember: making an API request looks a lot like typing a specially-formatted URL. That's kind of what it is. But instead of getting a rendered HTML web page in return, you get some data in return.\n",
        "\n",
        "There are a few different ways that we can query the Genius API, all of which are discussed in the [Genius API documentation](https://docs.genius.com/#/getting-started-h1). (In general, an API's documentation will explain how to use the API.) The way we're going to cover in this lesson is the [basic search](https://docs.genius.com/#songs-h2), which allows you to get a bunch of Genius data about any artist or songs that you search for, and it looks something like this:\n",
        "\n",
        "`http://api.genius.com/search?q={search_term}&access_token={client_access_token}`\n",
        "\n",
        "Let's break it down. But first, we need to: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkyAi9HT4uxG"
      },
      "outputs": [],
      "source": [
        "import requests # requests again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFEcigxL4uxG"
      },
      "source": [
        "Then we need the base URL for the Genius API. We'll assign it like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af_-wtRY4uxH"
      },
      "outputs": [],
      "source": [
        "base_url = \"http://api.genius.com\" # this is the URL for the Genius API; we're just storing it as a string\n",
        "base_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXwQpMt4uxH"
      },
      "source": [
        "Up next, we add '/search', which is what we learned about from reading the documentation. It tells the Genius API that we want to do a basic search. We'll add it to the end of the base_url (which is just a string) like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LVpZqE84uxH"
      },
      "outputs": [],
      "source": [
        "search_url = base_url + \"/search\" \n",
        "search_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNEMCYAb4uxI"
      },
      "source": [
        "Next, we have '?q={search term}'. \n",
        "\n",
        "The \"q\" is Genius's search paramater; it tells Genius that what follows is what we're searching _for_. Let's search for Beyonc√©'s mew single, \"Break My Soul.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NanF8w3h4uxI"
      },
      "outputs": [],
      "source": [
        "search_term = \"Break My Soul\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl8rCqfd4uxI"
      },
      "source": [
        "Finally, we have '&access_token={client_access_token}'. You've already defined this term above with your own token!\n",
        "\n",
        "We can put it all back together now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoiPMw9c4uxI"
      },
      "outputs": [],
      "source": [
        "genius_search_url = f'http://api.genius.com/search?q={search_term}&access_token={client_access_token}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJkefvj_4uxJ"
      },
      "source": [
        "But wait? What's that 'f' doing in front of the URL? \n",
        "\n",
        "This yet another way of formatting strings, known as a [formatted string literal or f-string](https://cito.github.io/blog/f-strings/). \n",
        "\n",
        "What it means is that, if you preface a string with an \"f\", any variables placed in curly braces ( `{}` ) will be interpreted inline. So in this case, {search_term} will be replaced by our search_term, and {client_access_token} will be replaced by our client_access_token.\n",
        "\n",
        "Note that you could *also* do: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcEymEkG4uxJ"
      },
      "outputs": [],
      "source": [
        "genius_search_url2 = search_url + \"?q=\" + search_term + \"&access_token=\" + client_access_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhWRgsfu4uxJ"
      },
      "source": [
        "But in this case the f-string is a bit more legible.\n",
        "\n",
        "So now here we go with the API call!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ciidjaz4uxJ"
      },
      "outputs": [],
      "source": [
        "# and here's the API call\n",
        "resp = requests.get(genius_search_url)\n",
        "data = resp.json()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTNpu1RS4uxK"
      },
      "source": [
        "This request is finding all songs that include the search string `Break My Soul`. \n",
        "\n",
        "As described in the [documentation](https://docs.genius.com/#/response-format-h1), the results take the form of a dictionary with two keys: `response` (which points to a dictionary of a list of dictionaries; phew!) and `meta`, whose value is a string (`'status'`), which gives you the HTML status code for the response (i.e. whether the request was successful). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ_O228E4uxK"
      },
      "source": [
        "Because the response is a dictionary, we can isolate the two top-level keys to get an overall view of the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76BNQH404uxK"
      },
      "outputs": [],
      "source": [
        "data.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_VKail_4uxK"
      },
      "source": [
        "So we know that the response was successful. \n",
        "\n",
        "But let's dig a little deeper into the `response` key. It itself is a dictionary, so we can look at _its_ keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u8MDbsM4uxL"
      },
      "outputs": [],
      "source": [
        "data['response'].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmatqIvC4uxL"
      },
      "source": [
        "So there is only one key, `hits`, which I will tell you contains a _further_ list of dictionaries: one for each of the hits in the search result.\n",
        "\n",
        "Let's take a look at the first result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwP1gny34uxL"
      },
      "outputs": [],
      "source": [
        "data['response']['hits'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "631E5zwW4uxL"
      },
      "source": [
        "So this is what we want: the dictionary for each of the search results.\n",
        "\n",
        "But lo and behold, it contains additional levels of data, and they each appear to be dictionaries! \n",
        "\n",
        "Three of the four-- `highlights`, `index`, and `type`-- each only have one item.\n",
        "\n",
        "But the `result` dictionary is where the good stuff is. \n",
        "\n",
        "Important items in this dictionary are the song title itself (`title`), the URL for the song lyrics (`url`), and the `primary artist` key, which points to *another* dictionary with the name of the artist (`name`). \n",
        "\n",
        "The artist name could be used with a different API endpoint to get more detail about a particular artist. But this information is enough for our purposes at the moment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIMLFYPR4uxL"
      },
      "source": [
        "To get a more compact view of the results of our initial query, for song titles with \"Break My Soul\" in them, let's see if we can print out the full song title for each search hit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCNV9jPx4uxL"
      },
      "outputs": [],
      "source": [
        "# Remember list comprehension format: [ predicate expression FOR temporary variable name IN source list ]\n",
        "\n",
        "titles = [song['result']['title'] for song in data['response']['hits']]\n",
        "\n",
        "titles\n",
        "\n",
        "# This means, for each song in data['response']['hits'], add its ['result']['title'] to a new list called \"titles\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRAoY3lA4uxM"
      },
      "source": [
        "**Question:** What key would we change to list the URLs for the lyrics of each of these songs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh_ZkWFP4uxM"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zs4Y4k4uxM"
      },
      "source": [
        "**Exercise:** Adapting the syntax above, list the name of the artist for each of these songs.\n",
        "    \n",
        "**Hint:** Remember that the artist `name` is contained *within* the dictionary `primary artist`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5JsQ49U4uxM"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK-nNCPx4uxM"
      },
      "source": [
        "### Working with responses\n",
        "\n",
        "Now we have a response from the API, and we've parsed it into a Python data structure that we know how to use (a dictionary). But now what do we do with it?\n",
        "\n",
        "In some cases, the response from the API contains all of the data that you need to create your dataset. But in other cases, you need to chain together additional information gained from an API call with another API call--or, in yet other cases, with some web scraping. \n",
        "\n",
        "In this case, you'll notice that the response contains an item, `url`, that contains a link to a URL with the song's lyrics. But it doesn't actually provide the lyrics themselves. This has to do with the sad reality that most for-profit companies don't want to give away their most valuable data for free. \n",
        "\n",
        "It turns out that Genius has made their lyrics data increasingly difficult to access. But if we *wanted* to create a song lyrics dataset that contained \"Break My Soul\"... "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qvc3G6igL9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7Y6Ma2egMLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ci072edOgMUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping on the Actual Web: 2022 Edition "
      ],
      "metadata": {
        "id": "0N_suO-4HgYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with what we know how to do: finding the URL for the lyrics for Beyonc√©'s \"Break My Soul\"\n",
        "\n",
        "Remember that we've already got our `data` stored from our API call. \n",
        "\n",
        "Now let's create a variable for our artist name:"
      ],
      "metadata": {
        "id": "d6UIkV55gM04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyzDf3JQ4uxM"
      },
      "outputs": [],
      "source": [
        "# Create a placeholder list to hold any matches\n",
        "lyrics_url = []\n",
        "\n",
        "# Use our data variable, populated with info from the API, to pull up the URL we need to scrape \n",
        "for song in data['response']['hits']:\n",
        "    if song['result']['primary_artist']['name'] == \"Beyonc√©\":\n",
        "        lyrics_url.append(song['result']['url']) # appending to list \n",
        "\n",
        "lyrics_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iOYndvt4uxN"
      },
      "source": [
        "Looks like there are two matches, but the first seems more authoritative. Let's use that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = lyrics_url[0]\n",
        "\n",
        "url"
      ],
      "metadata": {
        "id": "M0s5wyCKpICe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step: See if we can get the contents of the page at that URL\n",
        "\n",
        "**Does anyone remember the first step?**\n",
        "\n",
        "Hint: it involves the `requests` library"
      ],
      "metadata": {
        "id": "gDfO4wuZH4NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(url)\n",
        "\n",
        "response\n"
      ],
      "metadata": {
        "id": "tG1wxSXKHWBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13K2gCjqOrtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note about colab, we'll eventually load the page locally instead "
      ],
      "metadata": {
        "id": "oguhG18hOrv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "az_8pZnfHWEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*NB: In shifting over from Jupyter to Colab, I encountered my first issue with the platform, which is that Genius.com does not like that http requests are originating from somewhere in the Google cloud, and suspects the worst. As a workaround for today's class, we'll just load a static version of the document to use for the rest of the lesson. But do remember in the future if you are getting 403 errors and no one else on the internet who is trying to do the same thing as you seems to be getting them, try downloading your notebook and running it locally on your laptop. That worked for me!* "
      ],
      "metadata": {
        "id": "CIOpLf6byQ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the static html instead\n",
        "response = requests.get('https://raw.githubusercontent.com/laurenfklein/QTM340-Fall22/main/corpora/lyrics/Beyonce-break-my-soul-lyrics.html')"
      ],
      "metadata": {
        "id": "S-faWiPsHWKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've gotten the contents of the page, it's a good idea to take a look.\n",
        "\n",
        "**How can you print the response from the server as text?**"
      ],
      "metadata": {
        "id": "BNyKLstTIjKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "wrv0zHFVHWM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvq2UZdsHWPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BePqe_K8HWR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whoa! That's a lot more complicated than kittens! Let's go back to Chrome and [take a look](https://genius.com/Beyonce-break-my-soul-lyrics) using Developer Tools.\n"
      ],
      "metadata": {
        "id": "0IpMimbBIytS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-kvkAwSdJLcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAj0_Z5LJLfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nyt8d8YqJLiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a lay of the (HTML) land, try doing a command-f for \"intro: big freedia\" in the developer window, since that's some text that seems to start the portion of the page that has the lyrics. \n",
        "\n",
        "**First round of questions for the class**:\n",
        "* What is the tag enclosing the phrase \"intro: big freedia\"? \n",
        "* Does this tag have any attributes, and if so, what are they?"
      ],
      "metadata": {
        "id": "xl_q4OgAJ_zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "HlgbhqDOK5VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVcoBNHUK5bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7WOgg0tK5d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhgbnQwaK5gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second set of questions:**\n",
        "* Is there only one div with the attribute \"data-lyrics-container=true\", or are there many?\n",
        "* What BeautifulSoup method should we use to ensure that we get the appropriate number of div tag(s)?"
      ],
      "metadata": {
        "id": "Zm7EdiUlLzDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to import BeautifulSoup since we haven't yet used it in this notebook\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# now let's use BeautifulSoup to parse the html document that we got using requests just a few minutes ago\n",
        "\n",
        "document = BeautifulSoup(html_str, \"html.parser\")\n",
        "\n",
        "# and your BeautifulSoup query goes here... \n",
        "\n"
      ],
      "metadata": {
        "id": "A8UKDGatMntD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "atyKtomTzsvt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPY3LO4b99Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLNUP0c399IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6hN_OQbQ99K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# does not work! \n",
        "lyrics_divs.string"
      ],
      "metadata": {
        "id": "Uzc132Dr2-iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# does work, sorta!\n",
        "lyrics_divs.get_text()"
      ],
      "metadata": {
        "id": "CpGDnY5H3JfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0aqpjfS3O_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's not perfect, but it's good enough for now! \n",
        "\n",
        "## A Quick Note on API Wrappers\n",
        "\n",
        "An API wrapper is a package that makes an API easier to use and/or extends the API‚Äôs functionality. \n",
        "\n",
        "For example, a data scientist named John Miller wrote a Python package called [LyricsGenius](https://github.com/johnwmillr/LyricsGenius), which makes working with the Genius API easier and adds functionality not offered by the Genius API, including scraping lyrics (but it also doesn't work via Colab).\n",
        "\n",
        "The Twitter API has something called [twarc2](https://twarc-project.readthedocs.io/en/latest/twarc2/), which is the equivalent for the Twitter API. \n",
        "\n",
        "And the best thing to use for Reddit data is the [PushShift API Wrapper](https://github.com/dmarx/psaw), or PSAW. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3PQE6HVIcU30"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqL8CfDZMn_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And a bit more on legal / ethical considerations, via Melanie Walsh\n",
        "\n",
        "### Legal considerations\n",
        "\n",
        "If internet data is publicly available (e.g., tweets from a public Twitter account), it is generally considered legal to collect this data, even if a particular platform says that you cannot. In 2019, the Ninth Circuit Court of Appeals ruled that scraping publicly accessible websites likely does not violate federal anti-hacking laws. You can read more about [this legal ruling](https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data#:~:text=Linkedin%20Protects%20Scraping%20of%20Public%20Data,-Share%20It%20Share&text=In%20a%20long%2Dawaited%20decision,and%20Abuse%20Act%20(CFAA)) from the Electronic Frontier Foundation.\n",
        "\n",
        "### Institutional Review Boards (IRBs)\n",
        "\n",
        "Research that involves human participants (e.g., surveys, interviews, blood draws) needs to be approved by an Institutional Review Board (IRB). But research about publicly available internet data does not typically require IRB approval.\n",
        "\n",
        "### Publishing, Privacy, and Citation \n",
        "\n",
        "Just because something is legal or gets approved by an IRB does not mean it is ethical. Collecting, sharing, and publishing internet data created by or about individuals can lead to unwanted public scrutiny, harm, and other negative consequences for those individuals. For these reasons, some researchers attempt to anonymize internet data before sharing it or before publishing an article that cites a post specifically. Yet anonymizing internet data also does not give credit to internet users as creators and authors.\n",
        "\n",
        "There is no single, simple answer to the many difficult questions raised by internet data collection. It is important to develop an ethical framework that responds to the specifics of your particular research project or use case (e.g., the platform, the people involved, the context, the potential consequences, etc.).\n",
        "\n",
        "In any published research, you may want to consider seeking explicit permission from internet users when you want to quote them in an article, or only share internet data that meets a certain threshold of publicness, such as tweets from verified Twitter accounts or Reddit posts with a certain number of upvotes. \n",
        "\n",
        "### Models & Examples of Social Media Data in Published Research\n",
        "\n",
        "Below are a few examples of how researchers have approached social media data in published research:\n",
        "\n",
        "### Paraphrasing Posts\n",
        "In Maria Antoniak, David Mimno, and Karen Levy‚Äôs [article about a Reddit subcommunity dedicated to birthstories (r/BabyBumps)](https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf), which we will read later this semester, they paraphrased Reddit submissions discussed in the article and then deleted all collected Reddit data after the article was published.\n",
        "\n",
        "### Linking to Posts & Using ‚ÄúReasonably Public‚Äù Thresholds\n",
        "In Deen Freelon, Charlton McIlwain, and Meredith D. Clark‚Äôs [report about the #BlackLivesMatter movement](https://cmsimpact.org/wp-content/uploads/2016/03/beyond_the_hashtags_2016.pdf), they included links to tweets rather than the full text of tweets and only linked to tweets with a minimum of 100 retweets published by Twitter users who had at least 3,000 followers or were verified. They embargoed their Twitter data for a year and then publicly released a list of tweet IDs. Tweet IDs can be used by third-parties to re-download any tweets that have not been deleted yet.\n",
        "\n",
        "### Direct Collaboration & Conversation with Users\n",
        "In Emory alum Moya Bailey‚Äôs [article about the #GirlsLikeUs hashtag](http://www.digitalhumanities.org/dhq/vol/9/2/000209/000209.html), created by trans advocate Janet Mock, she asked for Mock‚Äôs permission to work on the project before it began and collaborated with Mock to develop research questions and determine the project‚Äôs direction.\n",
        "\n",
        "## Additional Recommended Reading\n",
        "\n",
        "* [Doc Now White Paper](https://www.docnow.io/docs/docnow-whitepaper-2018.pdf), Bergis Jules, Ed Summers, Dr. Vernon Mitchell, Jr.\n",
        "* [No Robots, Spiders, or Scrapers: Legal and Ethical Regulation of Data Collection Methods in Social Media Terms of Service](https://cmci.colorado.edu/~cafi5706/ICWSM2020_datascraping.pdf), Casey Fiesler, Nathan Beard, Brian C. Keegan\n",
        "* [#transform(ing)DH Writing and Research: An Autoethnography of Digital Humanities and Feminist Ethics](http://www.digitalhumanities.org/dhq/vol/9/2/000209/000209.html), Moya Bailey\n",
        "* [The #TwitterEthics Manifesto](https://modelviewculture.com/pieces/the-twitterethics-manifesto), Dorothy Kim and Eunsong Kim"
      ],
      "metadata": {
        "id": "ffYvTuJp7in_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*I wrote version 1.0 of this notebook in Fall 2019. It has since been supplemented with material from Melanie Walsh's chapter [Song Genius API](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Data-Collection/Genius-API.html) from her online textbook [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/welcome.html) as well as from Prof. Dan Sinykin's 2020 iteration of QTM 340. I last revised this notebook in Fall 2022*.\n",
        "\n"
      ],
      "metadata": {
        "id": "PEyi7VSpX88u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwPhApBPX9a8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}