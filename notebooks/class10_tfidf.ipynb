{"cells":[{"cell_type":"markdown","metadata":{"id":"EDtvbdozFGPC"},"source":["# tf-idf\n","\n","We will get to machine learning soon. But in many cases, just counting words can tell you a lot. To wit:\n","\n","<img src=\"http://lklein.com/wp-content/uploads/2021/10/Screen-Shot-2021-10-06-at-3.33.34-PM.png\">\n","\n","Today, we're going to explore a method called Term Frequency - Inverse Document Frequency (tf-idf). Tf-idf comes up a lot in text analysis projects because it’s both a corpus exploration method and a pre-processing step for many other text-mining measures and models.\n","\n","The procedure was introduced in a 1972 paper by Karen Spärck Jones under the name “term specificity,” and the basic idea is this:\n","\n","Instead of representing a term in a document by its raw frequency or its relative frequency (the term count divided by the document length), each term is *weighted* by dividing the term frequency by the number of documents in the corpus containing the word. \n","\n","The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in any particular document are often the most frequently used words in all of the documents.\n","\n","By contrast, terms with the highest tf-idf scores are the terms in a document that are distinctively frequent in a document when that document is compared other documents. When you sort by tf-idf score, these distinctive terms rise to the top. "]},{"cell_type":"markdown","metadata":{"id":"4rJiI3FjFGPL"},"source":["## An Analogy ##\n","    \n","If this explanation doesn’t quite resonate, a brief analogy might help. \n","\n","Say you've decided leave campus to get dinner on Buford Highway. Since leaving campus takes a lot of effort (and also, crucially, access to a car), the food better be worth it! That means you'll need to balance two competing goals:\n","\n","1) The food has to be really tasty; and also, crucially: \n","2) If you're going to go all the way out to Buford Highway, it better be something that you can't also get in Emory Village. Otherwise, why go to all the trouble of getting there?!\n","\n","Or, to give an example involving actual food: you don't want to go all the way out to Buford Highway to get pizza. Even if the pizza on Buford Highway is pretty tasty, you can get pizza anywhere in town. How can you find out what is distintively tasty on Buford Highway?  \n","\n","If you looked up the Yelp reviews for the all restaurants on Buford highway and sorted by score, you would get an answer to the question of what's the tastiest. But it still won't help solve the problem of what's *distintively tasty* on Buford Highway--like hot pot, for example, which is something that you can't get in Emory Village.   \n","\n","So you need a way to tell the difference between what's tasty and what's distinctively tasty. To do so, you need to distinguish between four categories of food. Food that, on Buford Highway, is:\n","\n","- both tasty and distinctive (e.g. hot pot)\n","- tasty but not distinctive (e.g. pizza) \n","- distinctive but not tasty (e.g. tacos-- tho I'm open to disagreement here)\n","- neither tasty nor distinctive (e.g. Taco Bell--again, open to disagreement).\n","\n","These categories are what tf-idf helps you measure. Term frequencies can be assessed according to the same criteria. A term might be:\n","\n","- Frequently used in the corpus, and used especially frequently in one particular document <-- Interesting! \n","- Frequently used in the corpus, but used frequently in equal measure across all documents <-- Less interesting\n","- Infrequently used in the corpus, but nonetheless used frequently in one particular document <-- Potentially interesting\n","- Infrequently used in in the corpus and also infrequently used in the corpus consitently across all documents <-- Not interesting\n","\n","It's the words that are especially frequent in one document that are most interesting to us, and the ones that TF/IDF helps us identify. To see how, let's take a look at our next corpus--a slightly bigger one--articles published in the *Emory Wheel*."]},{"cell_type":"markdown","metadata":{"id":"ovRWSCyjFGPL"},"source":["## *The Emory Wheel* "]},{"cell_type":"markdown","metadata":{"id":"UAkuQmhJFGPM"},"source":["In this lesson, we're going to use tf-idf to study the articles published by *The Emory Wheel* betweeen 2014 and 2019. This dataset was created by Honggang Min and Kexin Guan for their final project in the 2019 iteration of this course, and was generously transfered back to me for future class use.  "]},{"cell_type":"markdown","metadata":{"id":"4g9NfJ3zFGPM"},"source":["## Pre-processing: prepare the documents\n","\n","Tf-idf works on sets of documents. We'll be using another (new to us) library, scikit-learn, in order to count the words. But before we do, we'll need to get the documents into a list, with each document stored as its own string. \n","\n","In this particular case, the documents are individual .txt files that are stored in a zip file on my Google Drive. Below is some code to get the data from Google Drive, unzipped, and formatted into a list for processing. \n","\n","Note that while this is custom code written for this particular dataset, you'll usually need to write some sort of file/text pre-processing code in order to use any particular library/method/tool. You'll get very familiar with writing code like this by the end of the course! "]},{"cell_type":"code","source":["# For downloading large files from Google Drive\n","# https://github.com/wkentaro/gdown\n","import gdown\n","\n","# then download the zip file\n","gdown.download('https://drive.google.com/uc?export=download&id=1SUWUVswaY_RDLhzFruQIDJe-i6I3gznC', quiet=False) "],"metadata":{"id":"6g---LWfQavS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unzip it \n","!unzip wheel-clean.zip"],"metadata":{"id":"Y1wGcpxVReXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"KpRMaDrMFGPM"},"outputs":[],"source":["# import this library for directory/file manipulation\n","import os\n","\n","# set the base directory -- note that this may need to change if you've saved a copy\n","# of this notebook elsewhere \n","base_dir = \"wheel-clean/\"\n","\n","# read in a list of all the filenames \n","docs = os.listdir(base_dir)\n","\n","# a list for storing the text of all the docs\n","wheel_docs = []\n","\n","# iterate through each of the docs in the directory\n","for doc in docs:\n","    with open(base_dir + doc, \"r\") as file:     # open the doc file \n","        text = file.read()                      # read the contents of the file \n","        wheel_docs.append(text)                   # append the contents of the file to our\n","                                                # all_docs list for future manipulation\n","\n","# just take a look at the first item to be sure it worked\n","print(\"Filename: \" + str(docs[0]) + \"\\n\") \n","print(wheel_docs[0])"]},{"cell_type":"markdown","metadata":{"id":"RXRqmpZbFGPO"},"source":["## To the TF-IDF calculation... or not (yet)!"]},{"cell_type":"markdown","metadata":{"id":"Pl0Q4UiEFGPO"},"source":["Like many other methods, we can calculate tf-idf with just a few lines of code. But because the calculation relies on a process that is, in itself, a pre-processing step for many future methods, and is in itself a little heady, we're going to spend some time loooking under the hood.\n","\n","So without further ado, introduing.... the `CountVectorizer`!\n","\n","## But wait... the count what? \n","\n","Thus far, we've examined words in terms of their grammar and syntax. We've also looked at words in terms of various units: the word, the line, the document, etc. And we've counted the numbers of each of these. \n","\n","From here on out, however, we'll be taking a different approach. We'll be turning the words themselves into numbers, and then applying statistical measures and models to the numbers that represent the words. TF-IDF, as well as machine learning techniques including topic modeling, BERT, similarity, classification, and clustering--essentially, the set of methods we'll be learning in the second half of the course--all rely on this basic transformation.\n","\n","## Introducing scikit-learn! \n","\n","We've now reached another milestone--our first use of scikit-learn (often abbreviated as sk-learn), Python's major machine learning library, which also happens to be crucial to many of the more advanced methods named above. \n","\n","We're going to use this to transform our words into numbers.\n","\n","We'll begin by importing sk-learn's `CountVectorizer`, which [converts a collection of text documents into a matrix of token counts](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n","\n","I think I've used the word \"token\" in passing before, but here I'll take a minute to formally define it, along with some related terms.\n","\n","## Tokens, features, document-term matrices\n","\n","According to the [Stanford NLP group](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html), a *token* is \"an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\" The unit of the token is usually the word, but it can also be the sentence, the subword, or anything else that makes sense for that particular task.\n","\n","Take this famous phrase, for example:\n","\n","\"To be or not to be\"\n","\n","This line has six tokens: \"to\", \"be\", \"or\", \"not\", \"to\", \"be\".\n","\n","It has four features: \"to\", \"be\", \"or\", \"not\"\n","\n","But wait, what is a feature?\n","\n","In this case, a *feature* is a unique token in the corpus. (Caveat: features, like tokens, can actually be anything that makes sense for the task, but for the purposes of turning words into numbers, features are most often unique words, or \"terms,\" as they're also sometimes called).\n","\n","When sk-learn's CountVectorizer does its thing, it first *tokenizes* all of the documents in the corpus--that is, it breaks up each document into its individual tokens--and then then creates a *document-term matrix* that counts up how many times each term, or feature, appears in each document. \n","\n","For example, the document-term matrix for the line above might look something like this:\n","\n","|   | to | be | or | not |\n","|---|----|----|----|-----|\n","|   | 2  | 2  | 1  | 1   |\n","\n","\n","If we add in the second part of that phrase as a new document, we might get something like this:\n","\n","\n","|         | to | be | or | not | that | is | the | question |\n","|---------|----|----|----|-----|------|----|-----|----------|\n","| line 1  | 2  | 2  | 1  | 1   | 0    | 0  | 0   | 0        |\n","| line 2  | 0  | 0  | 0  | 0   | 1    | 1  | 0   | 0        |\n","\n","\n","But enough of vectorizing by hand; let's try it out using sk-learn!\n","\n","\n","## Importing sk-learn's CountVectorizer\n","\n","\n"]},{"cell_type":"code","source":["# import CountVectorizer from sk-learn\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"I6Pr_3h_XVXq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vectorize a teeny corpus\n","\n","Now let's vectorize a teeny corpus we can see:"],"metadata":{"id":"tA8FDpIUXY-5"}},{"cell_type":"code","source":["# here's our corpus: the first stanza of \"Persimmons\" in which each line is its own document\n","corpus = [\n","    'In sixth grade Mrs. Walker',\n","    'slapped the back of my head',\n","    'and made me stand in the corner',\n","    'for not knowing the difference',\n","    'between persimmon and precision.',\n","    'How to choose',\n","]\n","\n","# instantiate the CountVectorizer object\n","# note that this is the same conceptual process we used to instantiate\n","# the VADER sentiment analysis object, and the spaCy document object\n","cv=CountVectorizer()\n","\n","# this steps generates document-term matrix for the doc; \n","# it's required before you do almost anything else\n","dtm=cv.fit_transform(corpus)\n","\n","# this method gives us the feature names that the CountVectorizer vectorized:\n","features = cv.get_feature_names_out()\n","\n","# this method turns our doc-term matrix into an array that can be manipulated:\n","dtm_array = dtm.toarray()\n","\n","print(\"All of the features in our corpus:\")\n","print(str(features))\n","\n","print (\"\\nAnd their counts in each of the \\\"documents,\\\" each of which is really just a single line of the poem:\")\n","print(dtm_array)"],"metadata":{"id":"0xYmVnryXVQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# here is some code that uses dataframes to make the above slightly more legible\n","# note that this is now the second or third time I've mentioned Python dataframes\n","# and said we'll talk about them later--we will, promise!\n","\n","import pandas as pd\n","\n","df = pd.DataFrame(data=dtm_array,columns=features)\n","print(df)"],"metadata":{"id":"CIJqsZ-UXVNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a minute to figure out what we're looking at:\n","\n","* Each column is a feature, or \"term,\" labeled with the name of the term, which in this case is a unique token\n","* Each row is a document, labeled in order of being ingested\n","* The \"1\" in row 0 of the \"grade\" column means that the term \"grade\" appears 1 time in the first document... and so on.  \n","\n","## Vectorizing a corpus from a set of files\n","\n","The reality is that you almost always will be vectorizing a corpus from a set of files, and not a list that you type in by hand. This is how you'd do it with the Emory Wheel dataset that we loaded earlier:"],"metadata":{"id":"8hKSlbMyX1qI"}},{"cell_type":"code","source":["# instantiate the vectorizer, as before\n","cv=CountVectorizer()\n","\n","# generates document-term matrix for all the docs\n","dtm=cv.fit_transform(wheel_docs)\n","\n","# get the feature names aka terms\n","features = cv.get_feature_names_out()\n","\n","# take a look at the first 25 features\n","print(features[0:24])"],"metadata":{"id":"0c_514MXXVLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you can also check the overall shape of the doc-term matrix \n","dtm.shape"],"metadata":{"id":"fQ7TRvutXVFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What does this tell you about how many documents there are? What about the number of features?**"],"metadata":{"id":"-bCKz62UYqx9"}},{"cell_type":"code","source":[],"metadata":{"id":"FQhi3yU5Y1eO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XLFz7vmcY1aN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f0UgMJTQY1Qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VRnrqr-hXU3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Helpful CountVectorizer Parameters\n","\n","Since we're on the subject of the `CountVectorizer`, here are a few more helpful `CountVectorizer` parameters to know about:\n","\n"],"metadata":{"id":"NOCSfPW0ZBRu"}},{"cell_type":"code","source":["# lowercase all words -- this is True by default, but if you want to preserve case,\n","# you can set lowercase to False\n","cv_caps = CountVectorizer(lowercase=False)\n","\n","# generates document-term matrix for all the docs\n","dtm2=cv_caps.fit_transform(wheel_docs)\n","\n","# check the shape\n","dtm2.shape"],"metadata":{"id":"GFeW75S2XU2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So, there are more terms since it's not merging the uppercase and the lowercase versions of each word together.\n","\n","Another parameter to know about has to do with stopwords. These are common words like \"and\", \"not\", \"or\", etc. that are not usually that interesting."],"metadata":{"id":"rvBkKmTfZTbb"}},{"cell_type":"code","source":["# use the built-in English stopwords list\n","cv_no_stops = CountVectorizer(stop_words='english')\n","\n","# generates document-term matrix for all the docs\n","dtm3=cv_no_stops.fit_transform(wheel_docs)\n","\n","# check the shape\n","dtm3.shape"],"metadata":{"id":"Q7xjshObXUz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use a custom stopwrods list\n","cv_no_stops = CountVectorizer(stop_words=['emory','student','atlanta'])\n","\n","# generates document-term matrix for all the docs\n","dtm4=cv_no_stops.fit_transform(wheel_docs)\n","\n","# check the shape\n","dtm4.shape"],"metadata":{"id":"h0v9cqrUXUvb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One last helpful feature of CountVectorizer is that you can tell it very easily to tokenize by ngrams as well as words. To wit:"],"metadata":{"id":"PQapLcgOZn16"}},{"cell_type":"code","source":["bigram_cv = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","\n","# generates document-term matrix for all the docs\n","dtm5=bigram_cv.fit_transform(wheel_docs)\n","\n","# get the feature names -- bigrams in this case\n","features = bigram_cv.get_feature_names_out()\n","\n","# take a look at the first 25 features\n","print(features[0:24])"],"metadata":{"id":"EbS-iC8DXUmf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Converting the doc-term matrix to a dictionary\n","\n","Finally, here is some helpful code for creating a dictonary with the features as keys and the counts as values. Don't worry about being able to parse all the syntax unless you feel like it."],"metadata":{"id":"vkRYuqYGaB6-"}},{"cell_type":"code","source":["# here's our number of features\n","num_feats = dtm.shape[1]\n","\n","# here's a dictionary to store the features and counts key/value pairs\n","feature_dict = {}\n","\n","for x in range(num_feats):      # the for x in range() syntax is how you iterate over integers\n","    key = cv.get_feature_names_out()[x]  # this gets the feature name at position [x]\n","    value = dtm.toarray().sum(axis=0)[x]  # this sums the counts of the feature at\n","                                          # position [x] for all documents\n","    \n","    feature_dict[key] = value # add the new key/value pair to the dictionary\n","    \n","# then sort the dictionary in order of counts\n","sortFeats = sorted(feature_dict.items(), key=lambda x: x[1], reverse=True)\n","\n","# for more on the sorted function, see: https://www.w3schools.com/python/ref_func_sorted.asp\n","# for more on lambda functions, see: https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8\n","\n","# then print top 30\n","\n","for item in sortFeats[0:30]:\n","    print(str(item[0]) + \": \" + str(item[1]))"],"metadata":{"id":"qaqLNwqaZ9Js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Return to TF-IDF\n","\n","At long last, we're ready to calculate the TF-IDF scores for our corpus!\n","\n","We're going to do this step by step at first, to make sure that you understand each of the processes, and then at the end, you'll see how to do this in only a few lines of code."],"metadata":{"id":"zfne1yT-af5I"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"dFLnI8wGFGPO"},"outputs":[],"source":["# import the TF-IDF libraries\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"markdown","metadata":{"id":"xCVn3KaCFGPO"},"source":["## Create document-term matrix\n","\n","We'll use a doc-term matrix to calculate tf-idf. We already know how to do this step"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"aL3iCZvyFGPP"},"outputs":[],"source":["#instantiate CountVectorizer()\n","cv=CountVectorizer(stop_words='english') # using stopwords this time\n","\n","# this steps generates document-term matrix for the docs\n","dtm=cv.fit_transform(wheel_docs)\n","\n","# check shape\n","dtm.shape"]},{"cell_type":"markdown","metadata":{"id":"2DqZqMk7FGPP"},"source":["**How many articles do we have in this document-term matrix and how many unique features/terms?**"]},{"cell_type":"code","source":[],"metadata":{"id":"1hWsGNBXePp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bAHn_YzRePiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UVdLPubmePZE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2bmmtdUFGPP"},"source":["That line, as we learned last clas, tells us that we have 4008 rows, one for each document in the corpus, and 99,999 columns, one for each word (minus single character words, which the tokenizer excludes, as well as the default stopwords, which we've indicated should be excluded with the stop_words='english' parameter above).\n","\n","We can also look at the whole vocabulary like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7jqSKjVFGPP"},"outputs":[],"source":["cv.vocabulary_"]},{"cell_type":"markdown","metadata":{"id":"Vm20uYAMFGPQ"},"source":["The numbers above are the indices for each feature, not the word counts. But we can use the indicies in order to generate our word counts. \n","\n","Note that we did a similar thing above using `cv.get_feature_names_out()`. This is arguably more efficient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6C0lGcAUFGPQ"},"outputs":[],"source":["sum_words = dtm.sum(axis=0) # sum_words is a vector that contains the number of times each word appears in all \n","                            # the docs in the corpus. In other words, we are summing the elements for each column \n","                            # of the doc-term matrix and storing those counts as a vector \n","\n","# then sort the list of tuples that contain the word and their occurrence in the corpus.\n","# tuples are Python's name for single variables that actually store multiple variables, \n","# like the word and index in the vocabulary attribute above \n","\n","words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()] # rememeber list comprehension! \n","\n","words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n","\n","# display the top 10\n","words_freq[:10]"]},{"cell_type":"markdown","metadata":{"id":"K-8PinBOFGPQ"},"source":["We can already see some words with the most counts don't seem too distinctive: \"emory\" and \"students,\" for example. It's not surprising that those are the most frequently occurring words since the Wheel is a newspaper about Emory students.\n","\n","So now let's calculate the IDF values so that we can balance them out. While we could also calculate these by hand, sk-learn makes it really easy to do it in a few lines of code, so we'll use that instead. "]},{"cell_type":"markdown","metadata":{"id":"oCOR9fSPFGPQ"},"source":["## Initialize TfidfTransformer\n","\n","When you initialize TfidfTransformer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf. The recommended way to run `TfidfTransformer` is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in document length, and, overall, they'll produce more meaningful tf–idf scores. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"TJPPDspJFGPQ"},"outputs":[],"source":["# Call tfidf_transformer.fit on the word count vector we computed earlier.\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","tfidf_transformer.fit(dtm)"]},{"cell_type":"markdown","metadata":{"id":"vUYrCHl2FGPR"},"source":["## Print inverse document frequence (idf) values"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EMZTkxNIFGPR"},"outputs":[],"source":["# make a dataframe for the idf values\n","df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n"," \n","# sort ascending\n","df_idf.sort_values(by=['idf_weights'])"]},{"cell_type":"markdown","metadata":{"id":"bG5DaM5nFGPR"},"source":["In the table above, the words at the top are those that appear in the most number of documents, across all of the corpus; and the words at the bottom are those that appear in the least number of documents.\n","\n","Once again, it makes sense that words like \"Emory\" and \"said\" are at the top. It's a newspaper, after all! \n","\n","The words at the bottom appear to be either typos or whitespace errors. I'm guessing most of those appear only once across the entire corpus. \n","\n","## IDF by the numbers\n","\n","But what are these numbers that we're looking at?\n","\n","The most direct formula would be **N/df<sub>i</sub>**, where N represents the total number of documents in the corpus, and df is the number of documents in which the term appears. \n","\n","However, many implementations of tf-idf, including scikit-learn, which we are using, normalize the results with additional operations. \n","\n","In tf-idf, normalization is generally used in two ways, and for two reasons: first, to prevent bias in term frequency from terms in shorter or longer documents; and second, as above, to calculate each term’s idf value. \n","\n","Scikit-learn’s implementation of tf-idf represents N as **N+1**, calculates the natural logarithm of **(N+1)/df<sub>i</sub>**, and then adds **1** to the final result. Here is this same thing formatted slightly more nicely:\n","\n","<img src=\"http://lklein.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-02-at-11.52.31-PM.png\">\n","\n","**Important note!** This is only one way to calculate TF-IDF. There are many, many versions. The number itself isn't important. It's the *ranking* that the number enables that's most interesting to us. Because one you have the IDF values, you can now compute the tf-idf scores for any document or set of documents. \n","\n","So now let’s compute tf-idf scores for the documents in our corpus.\n"]},{"cell_type":"markdown","metadata":{"id":"uKBfk1n5FGPR"},"source":["## Produce & print tf-idf scores"]},{"cell_type":"markdown","metadata":{"id":"AlWSpEh1FGPR"},"source":["Once you have the idf values, you can compute the tf-idf scores for any document or set of documents. Let’s compute tf-idf scores for the documents in our corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"1up_4waUFGPS"},"outputs":[],"source":["# tf-idf scores\n","tf_idf_vector=tfidf_transformer.transform(dtm)"]},{"cell_type":"markdown","metadata":{"id":"iHntwARwFGPS"},"source":["Now, let’s print the tf-idf values of the first document to see if they make sense. \n","\n","We'll place the tf-idf scores from the first document into a pandas dataframe and sort the dataframe in descending order of scores."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"y13fcHeoFGPS"},"outputs":[],"source":["feature_names = cv.get_feature_names_out()\n","\n","#get tfidf vector for first document\n","first_document_vector=tf_idf_vector[0]\n"," \n","#print the scores for the first doc\n","df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n","df.sort_values(by=[\"tfidf\"],ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"UPpZCZi9FGPS"},"source":["Notice that only certain words have scores. This is because only the words in this document have a tf-idf score and everything else, from other documents, shows up as zeroes."]},{"cell_type":"markdown","metadata":{"id":"2sOGlaANFGPS"},"source":["## tf-idf: the fast way"]},{"cell_type":"markdown","metadata":{"id":"7JtgLxLLFGPS"},"source":["Since we're now tf-idf pros, we're going to use scikit-learn's all-in-one tf-idf vectorizer to do this entire notebook again in two lines of code. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"jLkZlv8JFGPS"},"outputs":[],"source":["tfidf_vectorizer=TfidfVectorizer(stop_words='english', use_idf=True) # excludings stopwords again\n"," \n","# send in all your docs here\n","tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(wheel_docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"it0IQaHHFGPT"},"outputs":[],"source":["# place tf-idf values for all docs in a pandas dataframe\n","# tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n","\n","tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names())\n","\n","tfidf_df"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"I-zrPKE4FGPT"},"outputs":[],"source":["# Add row for number of times word appears in all documents\n","tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()"]},{"cell_type":"markdown","metadata":{"id":"CJdO-zcmFGPT"},"source":["## Let's explore!"]},{"cell_type":"markdown","metadata":{"id":"N5IAAzoUFGPT"},"source":["We can look at specific words and how they appear in our newspaper corpus. I've entered five words below that we might want to investigate:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"zDoe_YUZFGPU"},"outputs":[],"source":["tfidf_slice = tfidf_df[['dooley', 'fall', 'pumpkin', 'spice', 'break']]\n","tfidf_slice"]},{"cell_type":"markdown","metadata":{"id":"T15vnZn3FGPU"},"source":["**Does this output make sense? What does it tell you about which articles you might want to go read? What about some research questions you might ask of this corpus using TF-IDF?**"]},{"cell_type":"markdown","metadata":{"id":"DsYyPHNqFGPU"},"source":["## Cosine similarity\n","\n","Just so you know how to do everything in that Pudding Hip-Hop feature, this is how you calculate cosine similiarty between documents on the basis of their tf/idf scores:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUP3kE_wFGPU"},"outputs":[],"source":["# CALCULATE SIMILARITY TO FIRST DOC \n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(tfidf_vectorizer_vectors[0:1], tfidf_vectorizer_vectors)"]},{"cell_type":"markdown","source":["*Lauren F. Klein wrote version 1.0 of this notebook in 2019 based of tutorials by [Matthew Lavin](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) and [Kavita Ganesan](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XZVlcOdKhSw). Dan Sinykin supplemented it with material from Melanie Walsh's chapter [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF.html) in 2020. Lauren Klein updated it again in 2021 and 2022.*\n","\n"],"metadata":{"id":"zwvKWaReNX2h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j60BQngDFGPU"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"provenance":[{"file_id":"1b8K4QbsGx3ZVX2ivGfvqEXTakkasy61g","timestamp":1664426449444}],"collapsed_sections":["bG5DaM5nFGPR"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}